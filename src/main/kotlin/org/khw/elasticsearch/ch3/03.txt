object 타입의 필드는 평탄화가 되어 필드명이 저장된다
    * object 배열 타입은 배열내 각 객체(원소)를 독립적으로 취급하지 않는다
"scope" : {
    "value"
} -> "scope.value"

bool 쿼리의 must 절에 쿼리를 여러 개 넣으면 각 쿼리가 AND 조건으로 연결된다.

GET object_test/_search
{
  "query" : {
    "bool" : {
      "must": [
        {
          "term" : {
            "spec.cores": "6"
          }
        },
        {
          "term": {
            "spec.memory": {
              "value": "128"
            }
          }
        }
      ]
    }
  }
}

object 타입의 배열은 배열을 구성하는 객체 데이터를 서로 독립적인 데이터로 취급하지 않는다
{
    "spec.cores" : [12,6,6],
    "spec.memory" : [128,64,32]
}
spec.cores가 6, spec.memory가 128 문서를 찾을려고 해도 일치하는 문서가 없는데 않는데 해당 object 원소를 가진 문서가 정상 조회 된다

nested 타입
    * nested 배열 타입은 배열내 각 객체(원소)를 독립적으로 취급한다
    * nested 타입은 객체 배열의 각 객체를 내부적으로 별도의 루씬 문서로 분리해 저장
        배열 원소가 100개라면 부모 문서까지 101개의 문서가 내부적으로 저장
    * object 타입 쿼리 조회랑 nested 타입 쿼리 조회랑 다르다

   GET nested_test/_search
   {
     "query": {
       "nested": {
         "path": "spec",
         "query" : {
           "bool" : {
             "must": [
               {
                 "term" : {
                   "spec.cores": "12"
                 }
               },
               {
                 "term": {
                   "spec.memory": {
                     "value": "128"
                   }
                 }
               }
             ]
           }
         }
       }
     }
   }

    * nested 타입은 내부적으로 각 객체를 별도의 문서로 분리해서 저장하기 때문에 성능 문제가 있을 수 있다
        index.mapping.nested_fields.limit 설정은 한 인덱스에 nested 타입을 몇 개까지 지정할 수 있는지를 제한 기본값은 50
        index.mapping.nested_objects.limit 설정은 한 문서가 nested 객체를 몇 개까지 가질 수 있는지를 제한 기본값은 10000 이 값을 무리하게 높이면 oom 위험있다

object 용도 : 일반적인 계층 구조에 사용
       성능 : 상대적으로 가볍nested보다
       검색 : 일반적인 쿼리 사용

nested 용도 : 배열 내 각 객체를 독립적으로 취급해야 하는 특수한 상황에서 사용
    성능 : 상대적으로 무겁다. 내부적으로 숨겨진 문서를 생성
    검색 : 전용 nested 쿼리로 감싸서 사용

그외 타입
    geo_point : 위도와 경도를 저장하는 타입
    geo_shape : 지도상에 특정 지점이나 선, 도형 등을 표현하는 타입
    binary : base64 로 인코딩된 문자열을 저자하는 타입
    long_range, date_range, ip_range : 경곗값을 지정하는 방법 등을 통해 수, 날짜, IP 등의 범위를 저장하는 타입
    completion : 자동완성 검색을 위한 특수한 타입

문자열 타입


PUT mapping_test
{
  "mappings": {
    "properties": {
      "createdDate" : {
        "type" : "date",
        "format" : "strict_date_time || epoch_millis"
      },
      "keywordString": {
        "type": "keyword"
      },
      "textString": {
        "type" : "text"
      }
    }
  }
}

    *text 타입은 애널라이저가 적용된 후 색인된다 들어온 문자열 값 그대로를 가지고 역색인을 구성하는 것이 아니라 값을 분석하여
        여러 토큰으로 쪼갠다. 이렇게 쪼개진 토큰으로 역색인을 구성한다 쪼개진 토큰에 지정한 필터를 적용하는 등의 후처리 작업 후 최종적으로 역색인에 들어가는 형태를 텀이라고 한다
    *keyword 로 지정된 필드에 들어온 문자열 값은 여러 토큰으로 쪼개지 않고 역색인을 구성한다
        애널라이저로 분석하는 대신 노멀라이저를 적용한다 노멀라이저는 간단한 전처리만을 거친 뒤 커다란 단일 텀으로 역색인을 구성

    * 애널라이저 = Hello, World! -> "hello" "world" 두개의 토큰으로 분리 후 색인
    * 노멀라이저 = Hello, World! -> "Hello, World!" 한개의 문자열로 색인

match 쿼리는 검색 대상 필드가 text 타입인 경우 검색 질의어도 애널라이저로 분석
GET mapping_test/_search
{
  "query" : {
    "match": {
      "textString": "the world said hello"
    }
  }
}
위와 같이 the world said hello라는 문자열로 검색을 수행하면
기본 standard 애널라이저가 질의러를 the, world, said, hello 4개의 텀으로 쪼갠다
그러므로 match 쿼리 수행 시 질의어를 쪼개 생성한 4개의 텀을 역색인에서 찾는다. world 텀과 hello 텀을 역색인에서 찾을 수 있으므로 검색 가능


*doc_values(text, annotated_text 사용 불가)
    디스크를 기반한 자료 구조로 파일 시스템 캐시를 통해 효율적으로 정렬, 집계, 스크립트 작업을 수행
    keyword 타입은 기본적으로 doc_values 캐시를 사용

*fielddata
    fielddata를 사용한 정렬이나 집계등의 작업 시에는 역색인 전체를 읽어들여 힙 메모리에 적재
    이러한 동작 방식은 힙을 순식간에 차지해 OOM 등 많은 문제를 발생시킬 수 있다.
    text 타입은 파일 시스템 기반의 캐시 doc_values 사용을 못하므로 fielddata캐시 사용 가능(기본 비활성화)

*_source
    _source 필드는 문서 색인 시점에 엘라스틱서치에 전달된 원본 JSON 문서를 저장하는 메타데이터 필드다
    문서 조회 API나 검색 API가 클라이언트에게 반환할 문서를 확정하고 나면 이 _source에 저장된 값을 읽어 클라이언트에게 반환 역색인을 생성하지 않기 때문에 검색 대상이 되지 않는다
    _source 필드는 JSON 문서를 통째로 담기때문에 디스크를 많이 사용한다 _source에 데이터를 저장하지 않도록 mappings에 설정할 수도 있다

    ㄴ _source 비활성화시 문제점
        1.update_by_query API를 이용할 수 없다. 엘라스틱서치의 세그먼트는 불변이다 업데이트 작업은 기존 문서를 변경하는 것이 아니라
          기존 문서를 삭제하고 업데이트된 새 문서를 색인하는 작업, 업데이트 작업은 이 과정에서 기존 문서의 내용을 확인해 문서의 내용을 병합후 새문서를 작성해야 하는데 _source가 없다면
          이러한 과정이 불가능
        2.reindex 불가능 reindex 작업도 _source의 원본 JSON 데이터를 읽어 재색인 작업을 수행

    ㄴ _source 비활성화 보단 인덱스 코덱(압축률)을 높이는 편이 낫다
        {
            "settings": {
                "index": {
                    "codec": "best_compression"
                }
            }
        }

    ㄴ synthetic source
        8.4 버전부터 도입된 기능 _source 비활성화와 다른 점은 _source를 읽어야 하는 때가 오면 문서 내 각 필드의 doc_values를 모아 _source를 재조립해 동작
        재조립된 _source도 원문 JSON 문자열과는 다를수 있다
        _source 비활성화와는 다르게 reindex 작업도 가능
        {
            "mappings" : {
                "_source": {
                    "mode": "synthetic"
                }
            }
        }

*index
    index 속성은 해당 필드의 역색인을 만들 것인지를 지정 기본값은 true
    false로 설정하면 해당 필드는 역색인이 없기 때문에 일반적인 검색대상이 되지 않는다
    역색인을 하지 않을뿐 doc_values를 사용하는 타입의 필드라면 정렬이나 집계의 대상으로는 사용 가능
    또한 완전히 검색 대상에서 제외되는게 아니라 역색인 대신 doc_values(결국 index를 사용하지 않는다면 doc_values를 사용하기 위해 keyword 타입등이 강제됨)를
     이용해 검색하므로 검색 성능이 떨어지지만 디스크 공간 절약 이점이 있다

     PUT mapping_test/_mapping
     {
       "properties": {
         "notSearchableText": {
           "type": "text",
           "index": false
         },
         "docValuesSearchableText": {
         "type": "keyword",
         "index": false
       }
       }
     }

*enabled
    enabled 설정은 object 타입의 필드에만 적용 enabled가 false로 지정된 필드는 엘라스틱서치가 파싱조차 수행하지 않는다
    데이터가 _source에는 저장되지만 다른 어느 곳에도 저장되지 않는다. 역색인을 생성하지 않기 때문에 검색, 정렬, 집계 불가능
    파싱을 수행하지 않기때문에 object 타입이 아닌 데이터가 들어와도 타입충돌 발생하지 않음
    여러 타입이 혼용되는 배열로 데이터 저장 가능

    PUT mapping_test/_mapping
    {
      "properties": {
        "notEnabled": {
          "type": "object",
          "enabled": false
        }
      }
    }

*애널라이저와 토크나이저
    애널라이저는 0개 이상의 캐릭터 필터, 1개의 토크나이저, 0개 이상의 토큰 필터로 구성된다
    캐릭터 필터를 적용해 문자열을 변형시킨 뒤 토크나이저를 적용해 여러 토큰으로 쪼갠다
    쪼개진 토큰의 스트림에 토큰 필터를 적용해 토큰에 특정한 변형을 가한 결과가 최종적으로 분석 완료된 텀

    *캐릭터 필터(텍스트 추가, 변경,삭제)
        캐릭터 필터는 텍스트를 캐릭터의 스트림으로 받아서 특정한 문자를 추가, 변경, 삭제
        여러 캐릭터 필터가 지정됐다면 순서대로 수행

    *내장 빌트인 캐릭터 필터
        HTML strip 캐릭터 필터: <b>와 같은 HTML 요소 안쪽의 데이터를 꺼낸다 &apos; 같은 HTML 엔티티도 디코딩
        mapping 캐릭터 필터: 치환할 대상이 되는 문자와 치환 문자를 맵 형태로 선언
        pattern replace 캐릭터 필터: 정규 표현식을 이용해서 문자를 치환

        POST _analyze
        {
          "char_filter": ["html_strip"],
          "text": "<p>I&apos;m so <b>happy</b>!<p>"
        }

        {
          "tokens": [
            {
              "token": """
        I'm so happy!
        """,
              "start_offset": 0,
              "end_offset": 31,
              "type": "word",
              "position": 0
            }
          ]
        }

    *standard 토크나이저
        Unicode Text Segmentation 알고리즘을 사용해 텍스트를 단어 단위로 나눈다
        대부분의 문장부호가 사라진다
        기본 애널라이저는 standard 그러므로 기본 토크나이저도 standard 토크나이저

        POST _analyze
        {
          "tokenizer": "keyword",
          "text": "Hello, HELLO, World!"
        }

    *ngram 토크나이저
        텍스트를 min_gram 값 이상 max_gram 값 이하의 단위로 쪼갠다
        예를 들면 min_gram 값을 2, max_gram 값을 3으로 지정한 뒤 hello라는 텍스트를 토크나이저로 쪼개면
        "he", "hel", "el", "ell", "ll", "llo" 라는 5개의 토큰으로 쪼개진다

        POST _analyze
        {
          "tokenizer": {
            "type": "ngram",
            "min_gram": 2,
            "max_gram": 3
          },
          "text": "Hello, World!"
        }

        ngram을 기본으로 사용시 불용어도 포함시키므로 토큰에 포함 시킬 타입의 문자를 지정가능
            - Letter : 언어의 글자로 분류되는 문자
            - Digit : 숫자로 분류되는 문자
            - whitespace : 띄어쓰기나 줄바꿈 문자 등 공백으로 인식되는 문자
            - punctuation : !나 " 등 문장 부호
            - symbol : $나 루트 같은 기호
            - custom : custom_token_chars 설정을 통해 따로 지정한 커스텀 문자

        POST _analyze
        {
          "tokenizer": {
            "type": "ngram",
            "min_gram": 2,
            "max_gram": 3,
            "token_chars": ["letter"]
          },
          "text": "Hello, World!"
        }

        min_gram, max_gram 차이는 기본이 1이다 이 옵션은 인덱스 생성시 index.max_ngram_diff

    *edge_ngram 토크나이저
        ngram 토크나이저와 유사한 동작을 수행하지만 모든 토큰의 시작 글자를 단어의 시작글자로 고정시켜서 생성한다
            ex) "Hello, World!" -> "Hel", "Hell", "Wor", "Worl" 토큰으로 쪼개지며 "ello" 같은 단어의 시작이 포함되지 않은 토큰은 생성X

        POST _analyze
        {
          "tokenizer": {
            "type": "edge_ngram",
            "min_gram": 2,
            "max_gram": 3,
            "token_chars": ["letter"]
          },
          "text": "Hello, World!"
        }

    *그 외 토크나이저
        - letter 토크나이저 : 공백, 특수문자 등 언어의 글자로 분류되는 문자가아닌 문자를 만났을때 쪼갠다
        - whitespace 토크나이저 : 공백 문자를 만났을때 쪼갠다
        - pattern 토크나이저 : 지정한 정규표현식을 단어의 구분자로 사용해 쪼갠다

    *토큰 필터(토큰 추가, 변경, 삭제)
        토큰 필터는 토큰 스트림을 받아서 토큰을 추가, 변경, 삭제한다. 하나의 애널라이저에 토큰 필터를 0개 이상 지정가능 여러개 지정되면 순차적 적용
            - lowercase / uppercase 토큰 필터: 토큰 내용을 소문자/대문자로 만들어준다
            - stop 토큰 필터: 불용어를 지저하여 제거할 수 있다 ex) the, a, an, in
            - synonym 토큰 필터: 유의어 사전 파일을 지정하여 지정된 유의어를 치환
            - pattern_replace: 정규식을 사용해 토큰의 내용을 치환
            - stemmer : 지원되는 몇몇 언어의 어간 추출을 수행 한국어는 지원 x
            - trim : 토큰의 전후에 위치한 공백문자 제거
            - truncate : 지정한 길이로 토큰을 자른다

        POST _analyze
        {
          "filter": ["lowercase"],
          "text": "Hello, World!"
        }

    *내장 애널라이저
        - standard 애널라이저 : standard 토크나이저와 lowercase 토큰 필터로 구성
        - simple 애널라이저 : letter가 아닌 문자 단위로 토큰을 쪼갠 뒤 lowercase 토큰 필터를 적용
        - whitespace 애널라이저 : whitespace 토크나이저로 구성, 즉 공백 문자 단위로 토큰 쪼갠다
        - stop 애널라이저 : standard 애널라이저와 같은 내용이지만 stop 토큰 필터를 적용해 불용어 제거
        - keyword 애널라이저 : keyword 토크나이저로 구성. 특별히 분석을 실시하지 않고 하나의 큰 토큰을 그대로 반환
        - pattern 애널라이저 : pattern 토크나이저와 lowercase 토큰 필터로 구성
        - fingerprint 애널라이저 : 중복 검출에 사용할 수 있는 핑거프린트용 토큰 생성
            standard 토크나이저 적용 뒤 lowercase 토큰 필터, ASCII folding 토큰 필터, stop 토큰 필터, fingerprint 토큰 필터를 차례대로 적용
            stop 토큰필터는 기본 비활성화, ASCII folding 토큰 필터는 ASCII내에 동격인 문자가 있는 경우 문자 치환, fingerprint 토큰 필터는 중복을 제거 단일 토큰 합침

        POST _analyze
        {
          "analyzer": "fingerprint",
          "text": "Yes yes, Godel said this sentence is consistent and."
        }

* 애널라이저를 인덱스 매핑에 적용
    PUT analyzer_test
    {
      "settings": {
        "analysis": {
          "analyzer": {
            "default" : {
              "type" : "keyword"
            }
          }
        }
      },
      "mappings": {
        "properties": {
          "defaultText": {
            "type": "text"
          },
          "standardText": {
            "type": "text",
            "analyzer": "standard"
          }
        }
      }
    }
    index.settings.analysis.analyzer 설정에는 커슽텀 애널라이저를 추가할 수 있다
    여기에 default 라는 이름으로 애널라이저를 지정하면 기본 애널라이저 변경 가능

* 커스텀 애널라이저
    커스텀 애널라이저는 캐릭터 필터, 토크나이저, 토큰 필터를 원하는 대로 조합해 지정 가능

    PUT analyzer_test2
    {
      "settings": {
        "analysis": {
          "char_filter": {
            "my_char_filter": {
              "type" : "mapping",
              "mappings": [
                "i. => 1.",
                "ii. => 2.",
                "iii. => 3.",
                "iv. => 4."
              ]
            }
          },
          "analyzer": {
            "my_analyzer": {
              "char_filter": [
                "my_char_filter"
                ],
                "tokenizer": "whitespace",
                "filter": ["lowercase"]
            }
          }
        }
      },
      "mappings": {
        "properties": {
          "myText": {
            "type": "text",
            "analyzer": "my_analyzer"
          }
        }
      }
    }

    *인덱스에 지정된 커스텀 애널라이저 테스트
        GET analyzer_test2/_analyze
        {
          "analyzer": "my_analyzer",
          "text": "i.Hello ii.World iii.Bye, iv.World!"
        }

    * 노멀라이저
        노멀라이저는 애널라이저와 비슷한 역할을 하나 적용 대상이 text 타입이 아닌 keyword 타입의 필드라는 차이, 단일 토큰 생성
        노멀라이저는 토크나이저 없이 캐릭터 필터, 토큰 필터로 구성, 애널라이저와 다르게 모든 캐릭터필터, 토큰 필터를 조합할 수 있는건 아니다
        단일 토큰 생성해야 하기 때문에 ASCII folding, lowercase, uppercase 글자 단위의 필터만 사용가능

        *커스텀 노멀라이저 적용 인덱스 생성(디폴트 노멀라이저는 lowercase 노멀라이저이다)
        PUT normalizer_test
        {
          "settings": {
            "analysis": {
              "normalizer": {
                "my_normalizer": {
                  "type": "custom",
                  "char_filter": [],
                  "filter" : [
                      "asciifolding",
                      "uppercase"
                    ]
                }
              }
            }
          },
          "mappings": {
            "properties": {
              "myNormalizerKeyword": {
                "type": "keyword",
                "normalizer": "my_normalizer"
              },
              "lowercaseKeyword": {
                "type": "keyword",
                "normalizer": "lowercase"
              },
              "defaultKeyword": {
                "type": "keyword"
              }
            }
          }
        }

        GET normalizer_test/_analyze
        {
          "field" : "myNormalizerKeyword",
          "text": "Happy World!!"
        }

        GET normalizer_test/_analyze
        {
          "field" : "lowercaseKeyword",
          "text": "Happy World!!"
        }


* 템플릿
    인덱스의 설정과 매핑을 사전에 정의한 설정대로 인덱스 생성 가능하게 한다

    PUT _index_template/my_template
    {
      "index_patterns": [
          "pattern_test_index-*",
          "another_pattern-*"
        ],
        "priority": 1, // priority 값을 이용하면 여러 인덱스 템플릿 간 우선 적용순위를 조정할 수 있다 높을 수록 우선순위 상승
        "template": {
          "settings": {
            "number_of_shards": 2,
            "number_of_replicas": 2
          },
            "mappings": {
              "properties": {
                "myTextField": {
                  "type": "text"
                }
              }
            }
        }
    }

    * 컴포넌트 템플릿
        템플릿 간 중복되는 부분을 재사용할 수 있는 작은 템플릿 블록으로 쪼갠 것이 컴포넌트 템플릿이다
        컴포넌트 템플릿은 다양한 인덱스 템플릿을 관리할 때 효율적

        PUT _component_template/timestam_mappings // mappings에 대한 템플릿
        {
          "template": {
            "mappings": {
              "properties": {
                "timestamp": {
                  "type": "date"
                }
              }
            }
          }
        }

        PUT _component_template/my_shard_settings // settings에 대한 템플릿
        {
          "template": {
            "settings": {
              "number_of_shards": 2,
              "number_of_replicas": 2
            }
          }
        }

        PUT _index_template/my_template2
        {
          "index_patterns": ["timestamp_index-*"], // 인덱스 생성패턴
          "composed_of": ["timestam_mappings", "my_shard_settings"] // 컴포넌트 템플릿 적용 목록
        }

    * 레거시 템플릿
        인덱스 템플릿과 컴포넌트 템플릿 API는 ES 7.8.0 버전부터 추가된 기능이다 이전 버전 템플릿 API는 레거시 템플릿이 됐다
        레거시 템플릿 API는 _index_template 대신에 _template을 사용하며 컴포넌트 템플릿 조합은 불가능 이외에 모든 기능 동일

    * 동적 템플릿
        인덱스에 새로 들어온 필드의 매핑을 사전에 정의한대로 동적 생성하는 기능
        동적 템플릿은 인덱스 템플릿과는 다르게 매핑 안에 정의 즉 인덱스를 생성할 땐 인덱스 템플릿을 생성할 때 함께 지정

        PUT _index_template/dynamic_mapping_template
        {
          "index_patterns": ["dynamic_mapping*"],
          "priority": 1,
          "template": {
            "settings": {
              "number_of_shards" : 2,
              "number_of_replicas": 2
            },
            "mappings": {
              "dynamic_templates": [
                {
                  "my_text": {
                    "match_mapping_type": "string",
                    "match": "*_text",
                    "mapping": {
                      "type": "text"
                    }
                  }

                },
                {
                  "my_keyword": {
                    "match_mapping_type": "string", // 데이터 타입을 JSON 파서를 이용해 확인한다
                    "match": "*_keyword", // 필드의 이름이 지정된 패턴과 일치하는지 확인
                    "mapping": {
                      "type": "keyword" // 타입 지정
                    }
                  }
                }
              ]
            }
          }
        }

* 빌트인 인덱스 템플릿
    ES 7.0.9 이상 버전은 미리 정의된 빌트인 인덱스 템플릿을 제공, 로그나 메트릭을 편리하게 수집하기 위한
    X-Pack 전용 추가 기능 Elastic Agent 에서 사요아기 위해 내장된 템플릿
    metrics-*-*와 logs-*-* 인덱스 패턴에 priority 값 100을 가진 템플릿이 사전 정의 되어 있다


* 라우팅
    ES가 인덱스를 구성하는 샤드 중 몇 번 샤드를 대상으로 작업을 수행할지 지정하기 위해 사용하는 값이다
    라우팅 값은 문서를 색인할 때 문서마다 하나씩 지정할 수 있다
    작업 대상 샤드 번호로 지정된 라우팅 값을 해시한 후 주 샤드의 개수로 나머지 연산을 수행행한 값이 된다
    라우팅 값을 지정하지 않고 문서를 색인할시 기본값은 _id, 색인 시 라우팅 값을 지정했다면 조회, 업데이트, 삭제, 검색등의 작업에서도 똑같은 라우팅을 지정해야한다

    PUT routing_test
    {
      "settings": {
        "number_of_shards": 5,
        "number_of_replicas": 1
      }
    }

    PUT routing_test/_doc/1?routing=myid
    {
      "login_id": "myid",
      "comment": "hello world",
      "created_at" : "2020-09-08T22:14:09.123Z"
    }
    GET routing_test/_search

    GET routing_test/_search?routing=myid

    * 인덱스 생성시 라우팅 값 필수 지정
        PUT routing_test{
            "mappings": {
                "_routing": {
                    "required": true
                }
            }
        }


